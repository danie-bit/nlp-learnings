{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danie-bit/nlp-learnings/blob/main/5_spacy_lang_processing_pipeline/spacy_pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vkn0PWPuv02t"
      },
      "source": [
        "<h2 align=\"center\">Spacy Language Processing Pipelines Tutorial</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvniW4Ufv02v"
      },
      "source": [
        "<h3>Blank nlp pipeline</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlXZ1jISv02w",
        "outputId": "9825ec6b-fef9-4e46-db9a-93df4a63fc8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain\n",
            "america\n",
            "ate\n",
            "100\n",
            "$\n",
            "of\n",
            "samosa\n",
            ".\n",
            "Then\n",
            "he\n",
            "said\n",
            "I\n",
            "can\n",
            "do\n",
            "this\n",
            "all\n",
            "day\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t0iEI54v02y"
      },
      "source": [
        "We get above error because we have a blank pipeline as shown below. Pipeline is something that starts with a Tokenizer component in a dotted rectange below. You can see there is nothing there hence the blank pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlPPpbxUv02y"
      },
      "source": [
        "<img height=300 width=400 src=\"https://github.com/codebasics/nlp-tutorials/blob/main/5_spacy_lang_processing_pipeline/spacy_blank_pipeline.jpg?raw=1\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNxKsC4iv02z",
        "outputId": "8f9298a1-aeda-4ce9-f9bc-f8b4c33becbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG7cLxAuv02z"
      },
      "source": [
        "nlp.pipe_names is empty array indicating no components in the pipeline. Pipeline is something that starts with a tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9yrg-QYv02z"
      },
      "source": [
        "More general diagram for nlp pipeline may look something like below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNGIXya-v02z"
      },
      "source": [
        "<img height=300 width=400 src=\"https://github.com/codebasics/nlp-tutorials/blob/main/5_spacy_lang_processing_pipeline/spacy_loaded_pipeline.jpg?raw=1\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zlw4TNGv020"
      },
      "source": [
        "<h3>Download trained pipeline</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeKNPky9v020"
      },
      "source": [
        "To download trained pipeline use a command such as,\n",
        "\n",
        "python -m spacy download en_core_web_sm\n",
        "\n",
        "This downloads the small (sm) pipeline for english language\n",
        "\n",
        "Further instructions on : https://spacy.io/usage/models#quickstart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMYbKuCbv020",
        "outputId": "229304f9-1625-478f-acb5-a1824eed9ff4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEjpcpzpv021",
        "outputId": "993134b9-89fb-429d-93a6-ecb6168f7eec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7d6f7b8ef530>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7d6f7c446f90>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7d6f7b9119a0>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7d6f7d0c1c10>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7d6f7c5fb190>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7d6f7b9118c0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nlp.pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRNoiAJRv021"
      },
      "source": [
        "sm in en_core_web_sm means small. There are other models available as well such as medium, large etc. Check this: https://spacy.io/usage/models#quickstart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6ZmTkuxv022",
        "outputId": "501f5bd6-60c7-4e71-dfab-60b916316fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain  |  proper noun  |  Captain\n",
            "america  |  proper noun  |  america\n",
            "ate  |  verb  |  eat\n",
            "100  |  numeral  |  100\n",
            "$  |  numeral  |  $\n",
            "of  |  adposition  |  of\n",
            "samosa  |  proper noun  |  samosa\n",
            ".  |  punctuation  |  .\n",
            "Then  |  adverb  |  then\n",
            "he  |  pronoun  |  he\n",
            "said  |  verb  |  say\n",
            "I  |  pronoun  |  I\n",
            "can  |  auxiliary  |  can\n",
            "do  |  verb  |  do\n",
            "this  |  pronoun  |  this\n",
            "all  |  determiner  |  all\n",
            "day  |  noun  |  day\n",
            ".  |  punctuation  |  .\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x36qyFRNv022"
      },
      "source": [
        "**Run same code above with a blank pipeline and check what output you see?**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_bl = spacy.blank(\"en\")\n",
        "doc = nlp_bl(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "for token in doc:\n",
        "    print(token, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BwqSHrBxIT9",
        "outputId": "6bc5cef5-3495-4814-ed00-ca1e68c13fc0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain  |  None  |  \n",
            "america  |  None  |  \n",
            "ate  |  None  |  \n",
            "100  |  None  |  \n",
            "$  |  None  |  \n",
            "of  |  None  |  \n",
            "samosa  |  None  |  \n",
            ".  |  None  |  \n",
            "Then  |  None  |  \n",
            "he  |  None  |  \n",
            "said  |  None  |  \n",
            "I  |  None  |  \n",
            "can  |  None  |  \n",
            "do  |  None  |  \n",
            "this  |  None  |  \n",
            "all  |  None  |  \n",
            "day  |  None  |  \n",
            ".  |  None  |  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/glossary.py:20: UserWarning: [W118] Term '' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNbF77a-v022"
      },
      "source": [
        "<h3>Named Entity Recognition</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ_WhuWjv022",
        "outputId": "62a48d9f-e287-46aa-e9a5-ecfce2258c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc | ORG\n",
            "$45 billion | MONEY\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "for ent in doc.ents:   # ents - entities\n",
        "    print(ent.text,\"|\", ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "W819zF7sv023",
        "outputId": "d3d461e8-10a1-4639-ff78-cc2d96a76774"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tesla Inc\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is going to acquire twitter for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $45 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"ent\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqJU7fftv023"
      },
      "source": [
        "<h3>Trained processing pipeline in French</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "v7ycZCs_v024",
        "outputId": "b8f0ae52-e227-4e99-d5a4-e96a611b5142"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'fr_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3247283718.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fr_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'fr_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "# nlp = spacy.load(\"fr_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2xPNzv8v024"
      },
      "source": [
        "You need to install the processing pipeline for french language using this command,\n",
        "\n",
        "python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqBKLoYuyI_-",
        "outputId": "15fb5fb6-6d56-484f-c996-8efa661fd1f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qod4Rd6Jv024"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"fr_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCgSzWj9v024",
        "outputId": "45850672-b70b-4ce3-f2db-4193f644074a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  |  PER  |  Named person or family.\n",
            "Twitter  |  MISC  |  Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtSkTFm8v025",
        "outputId": "542ee1f8-38d5-4722-e9d0-15894f601d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla  |  PROPN  |  Tesla\n",
            "Inc  |  PROPN  |  Inc\n",
            "va  |  VERB  |  aller\n",
            "racheter  |  VERB  |  racheter\n",
            "Twitter  |  VERB  |  twitter\n",
            "pour  |  ADP  |  pour\n",
            "$  |  NOUN  |  dollar\n",
            "45  |  NUM  |  45\n",
            "milliards  |  NOUN  |  milliard\n",
            "de  |  ADP  |  de\n",
            "dollars  |  NOUN  |  dollar\n"
          ]
        }
      ],
      "source": [
        "for token in doc:\n",
        "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9GSPUXv025"
      },
      "source": [
        "<h3>Adding a component to a blank pipeline</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghNJIuTTv025",
        "outputId": "a38d7d34-cd7f-4bdd-ef9a-b8e8019e5274"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "source_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"ner\", source=source_nlp)\n",
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uzrwwq_v025",
        "outputId": "0eb234a1-4f4c-460d-8a04-b7e297470840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc | ORG\n",
            "$45 billion | MONEY\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text,\"|\", ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7HZQ7_Gv026"
      },
      "source": [
        "In below image you can see sentencizer component in the pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrvbRjqsv026"
      },
      "source": [
        "<img height=300 width=400 src=\"https://github.com/codebasics/nlp-tutorials/blob/main/5_spacy_lang_processing_pipeline/sentecizer.jpg?raw=1\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBjBwor4v026"
      },
      "source": [
        "<h3>Further reading</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnPfrFftv026"
      },
      "source": [
        "https://spacy.io/usage/processing-pipelines#pipelines"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}